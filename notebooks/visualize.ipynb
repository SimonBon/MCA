{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "md-title",
   "metadata": {},
   "source": [
    "# MCA Experiment Visualisation\n",
    "\n",
    "End-to-end analysis of a training run from the `.npz` result files.\n",
    "\n",
    "| Section | What it shows |\n",
    "|---|---|\n",
    "| **1. Load Results** | Feature shapes, class distribution |\n",
    "| **2. Classification Metrics** | Accuracy table + confusion matrix |\n",
    "| **3. UMAP Embedding** | 2-D projection coloured by label / prediction / confidence |\n",
    "| **4. Marker Activations** | Per-marker feature means overlaid on UMAP |\n",
    "| **5. Per-class Marker Profile** | Heatmap of which markers drive each cell-type |\n",
    "\n",
    "> **Model attention maps** are covered in `attention_maps.ipynb`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md-cfg",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "Edit these variables before running the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-cfg",
   "metadata": {},
   "outputs": [],
   "source": "# ── Edit these ────────────────────────────────────────────────────────────────\nDATASET_NAME = 'CODEX_cHL_CIM_VICReg'\nRUNS_DIR     = '/home/simon_g/isilon_images_mnt/10_MetaSystems/MetaSystemsData/_simon/src/MCA/z_RUNS'\n\nN_TRAIN      = 10_000   # train samples to subsample for UMAP\nN_VAL        = 5_000    # val   samples to subsample for UMAP\nUMAP_DIMS    = 2        # 2 or 3\nFEAT_PER_CH  = 32       # feature channels per marker (must match model config)\nRANDOM_SEED  = 42\n\n# ── Runs to compare in Sections 6 & 7 ────────────────────────────────────────\n# Keys are display labels; values are folder names under RUNS_DIR.\nCOMPARE_RUNS = {\n    'CIM VICReg':   'CODEX_cHL_CIM_VICReg',\n    'ResNet VICReg': 'CODEX_cHL_ResNet_VICReg',\n}"
  },
  {
   "cell_type": "markdown",
   "id": "md-imports",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "import einops\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import umap\n",
    "from IPython.display import display\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "RUN_DIR = Path(RUNS_DIR) / DATASET_NAME\n",
    "print(f'Run directory: {RUN_DIR}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md-load",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Load Results\n",
    "\n",
    "The `.npz` files are written by `val_hook.py` after training.\n",
    "Each file contains features, string labels, top-1 / top-2 predictions, per-class logits, and sample IDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-load",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file = np.load(RUN_DIR / 'train_results.npz')\n",
    "val_file   = np.load(RUN_DIR / 'val_results.npz')\n",
    "\n",
    "train_features   = train_file['features']\n",
    "val_features     = val_file['features']\n",
    "train_labels_str = train_file['labels_str']\n",
    "val_labels_str   = val_file['labels_str']\n",
    "train_preds_str  = train_file['top1_pred_str']\n",
    "val_preds_str    = val_file['top1_pred_str']\n",
    "train_logits     = train_file['logits']   # (N, n_classes) – classifier probabilities\n",
    "val_logits       = val_file['logits']\n",
    "\n",
    "classes   = list(val_file['classes'])\n",
    "n_classes = len(classes)\n",
    "\n",
    "print(f'Dataset : {DATASET_NAME}')\n",
    "print(f'Classes : {classes}')\n",
    "print()\n",
    "print(f'Train  → {len(train_features):>7,} cells  |  feature dim: {train_features.shape[1]}')\n",
    "print(f'Val    → {len(val_features):>7,} cells  |  feature dim: {val_features.shape[1]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md-dist",
   "metadata": {},
   "source": [
    "### Class distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-dist",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(14, 4), sharey=False)\n",
    "\n",
    "for ax, labels, split in [\n",
    "    (axes[0], train_labels_str, 'Train'),\n",
    "    (axes[1], val_labels_str,   'Val'),\n",
    "]:\n",
    "    unique, counts = np.unique(labels, return_counts=True)\n",
    "    order = np.argsort(counts)[::-1]\n",
    "    bars = ax.bar(unique[order], counts[order], color='steelblue', edgecolor='white', linewidth=0.5)\n",
    "\n",
    "    # Label each bar with its count\n",
    "    for bar, cnt in zip(bars, counts[order]):\n",
    "        ax.text(\n",
    "            bar.get_x() + bar.get_width() / 2,\n",
    "            bar.get_height() + counts.max() * 0.01,\n",
    "            f'{cnt:,}', ha='center', va='bottom', fontsize=8\n",
    "        )\n",
    "\n",
    "    ax.set_title(f'{split} class distribution  (n={len(labels):,})', fontsize=11)\n",
    "    ax.set_xlabel('Cell type')\n",
    "    ax.set_ylabel('Count')\n",
    "    ax.tick_params(axis='x', rotation=40)\n",
    "    ax.spines[['top', 'right']].set_visible(False)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md-metrics",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Classification Metrics\n",
    "\n",
    "Metrics are computed by a logistic regression probe trained on the frozen features (see `val_hook.py`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-metrics-table",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_path = RUN_DIR / 'metrics.json'\n",
    "\n",
    "if metrics_path.exists():\n",
    "    with open(metrics_path) as f:\n",
    "        metrics = json.load(f)\n",
    "\n",
    "    rows = []\n",
    "    for split in ['train', 'val']:\n",
    "        m = metrics[split]\n",
    "        rows.append({\n",
    "            'Split':             split.capitalize(),\n",
    "            'Top-1 Acc':         f\"{m['top1_accuracy']:.3f}\",\n",
    "            'Top-2 Acc':         f\"{m['top2_accuracy']:.3f}\",\n",
    "            'Bal. Acc (top-1)':  f\"{m['top1_balanced_accuracy']:.3f}\",\n",
    "            'Bal. Acc (top-2)':  f\"{m['top2_balanced_accuracy']:.3f}\",\n",
    "            'F1 (weighted)':     f\"{m['f1']:.3f}\",\n",
    "            'N samples':         f\"{m['n_samples']:,}\",\n",
    "        })\n",
    "\n",
    "    display(pd.DataFrame(rows).set_index('Split').style.set_caption(\n",
    "        f'Logistic-regression probe — {n_classes} classes'\n",
    "    ))\n",
    "else:\n",
    "    print(f'metrics.json not found at {metrics_path}\\nCompute metrics inline:')\n",
    "    from sklearn.metrics import accuracy_score, balanced_accuracy_score, f1_score\n",
    "    for labels, preds, split in [\n",
    "        (train_labels_str, train_preds_str, 'Train'),\n",
    "        (val_labels_str,   val_preds_str,   'Val'),\n",
    "    ]:\n",
    "        acc  = accuracy_score(labels, preds)\n",
    "        bacc = balanced_accuracy_score(labels, preds)\n",
    "        f1   = f1_score(labels, preds, average='weighted')\n",
    "        print(f'  {split}: Acc={acc:.3f}  Bal.Acc={bacc:.3f}  F1={f1:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md-cm",
   "metadata": {},
   "source": [
    "### Confusion matrices\n",
    "\n",
    "Row-normalised (true label on y-axis, predicted on x-axis).  \n",
    "The diagonal is the per-class recall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-cm",
   "metadata": {},
   "outputs": [],
   "source": [
    "cell_size = max(0.7, 6 / n_classes)\n",
    "fig, axes = plt.subplots(1, 2, figsize=(cell_size * n_classes * 2 + 2, cell_size * n_classes + 1))\n",
    "\n",
    "for ax, labels, preds, title in [\n",
    "    (axes[0], train_labels_str, train_preds_str, 'Train'),\n",
    "    (axes[1], val_labels_str,   val_preds_str,   'Val'),\n",
    "]:\n",
    "    cm = confusion_matrix(labels, preds, labels=classes, normalize='true')\n",
    "    im = ax.imshow(cm, cmap='Blues', vmin=0, vmax=1)\n",
    "\n",
    "    for i in range(n_classes):\n",
    "        for j in range(n_classes):\n",
    "            ax.text(j, i, f'{cm[i, j]:.2f}',\n",
    "                    ha='center', va='center', fontsize=8,\n",
    "                    color='white' if cm[i, j] > 0.55 else 'black')\n",
    "\n",
    "    ax.set_xticks(range(n_classes))\n",
    "    ax.set_yticks(range(n_classes))\n",
    "    ax.set_xticklabels(classes, rotation=45, ha='right', fontsize=9)\n",
    "    ax.set_yticklabels(classes, fontsize=9)\n",
    "    ax.set_xlabel('Predicted')\n",
    "    ax.set_ylabel('True')\n",
    "    ax.set_title(f'{title} confusion matrix (row-normalised)', fontsize=11)\n",
    "    plt.colorbar(im, ax=ax, shrink=0.8)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md-umap",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. UMAP Embedding\n",
    "\n",
    "UMAP projects the high-dimensional features into 2-D for visual inspection.\n",
    "We subsample from the full split to keep the embedding tractable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-subsample",
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.default_rng(RANDOM_SEED)\n",
    "\n",
    "def _subsample(feat, labels, preds, logits, n):\n",
    "    idx = rng.permutation(len(feat))[:n]\n",
    "    return feat[idx], labels[idx], preds[idx], logits[idx]\n",
    "\n",
    "tr_feat, tr_lab, tr_pred, tr_logit = _subsample(\n",
    "    train_features, train_labels_str, train_preds_str, train_logits, N_TRAIN\n",
    ")\n",
    "va_feat, va_lab, va_pred, va_logit = _subsample(\n",
    "    val_features, val_labels_str, val_preds_str, val_logits, N_VAL\n",
    ")\n",
    "\n",
    "all_feat  = np.vstack([tr_feat, va_feat])\n",
    "all_lab   = np.concatenate([tr_lab,   va_lab])\n",
    "all_pred  = np.concatenate([tr_pred,  va_pred])\n",
    "all_split = np.array(['Train'] * len(tr_feat) + ['Val'] * len(va_feat))\n",
    "all_conf  = np.concatenate([tr_logit.max(1), va_logit.max(1)])  # top-class probability\n",
    "all_corr  = np.where(all_lab == all_pred, 'Correct', 'Incorrect')\n",
    "\n",
    "print(f'Subsampled: {len(tr_feat):,} train + {len(va_feat):,} val = {len(all_feat):,} total')\n",
    "print(f'Subsample accuracy: {(all_lab == all_pred).mean():.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-umap-fit",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Fitting UMAP…')\n",
    "reducer = umap.UMAP(\n",
    "    n_neighbors=15,\n",
    "    min_dist=0.05,\n",
    "    n_components=UMAP_DIMS,\n",
    "    metric='euclidean',\n",
    "    #random_state=RANDOM_SEED,\n",
    "    n_jobs=4,\n",
    "    verbose=True,\n",
    ")\n",
    "embedding = reducer.fit_transform(all_feat)\n",
    "print(f'Done. Embedding shape: {embedding.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-umap-df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a single DataFrame used by all downstream plots\n",
    "df = pd.DataFrame({\n",
    "    'x':          embedding[:, 0],\n",
    "    'y':          embedding[:, 1],\n",
    "    'label':      all_lab,\n",
    "    'predicted':  all_pred,\n",
    "    'correct':    all_corr,\n",
    "    'confidence': all_conf,\n",
    "    'split':      all_split,\n",
    "})\n",
    "if UMAP_DIMS == 3:\n",
    "    df['z'] = embedding[:, 2]\n",
    "\n",
    "# Consistent colour palette (same colour = same class, across all plots)\n",
    "label_colors = {\n",
    "    lab: px.colors.qualitative.Alphabet[i % len(px.colors.qualitative.Alphabet)]\n",
    "    for i, lab in enumerate(sorted(df['label'].unique()))\n",
    "}\n",
    "\n",
    "SCATTER_KW = dict(  # shared keyword arguments for scatter plots\n",
    "    x='x', y='y',\n",
    "    symbol='split', symbol_map={'Train': 'circle', 'Val': 'cross'},\n",
    "    hover_data=['label', 'predicted', 'correct', 'confidence', 'split'],\n",
    "    width=1100, height=700,\n",
    "    opacity=0.65,\n",
    ")\n",
    "\n",
    "def _style(fig):\n",
    "    fig.update_traces(marker=dict(size=4, line=dict(width=0)))\n",
    "    fig.update_layout(xaxis_title='UMAP-1', yaxis_title='UMAP-2')\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md-umap-plots",
   "metadata": {},
   "source": [
    "### 3a. Coloured by true cell type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-umap-label",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter(\n",
    "    df, color='label', color_discrete_map=label_colors,\n",
    "    title='UMAP — true cell type',\n",
    "    **SCATTER_KW\n",
    ")\n",
    "_style(fig).update_layout(legend_title='Cell type')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md-umap-corr",
   "metadata": {},
   "source": [
    "### 3b. Correct vs. incorrect predictions\n",
    "\n",
    "Green points are correctly classified; red points are errors.  \n",
    "Clusters of red points suggest confusion between specific cell types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-umap-correct",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put Correct on top so Incorrect (red) is not hidden\n",
    "df_sorted = pd.concat([\n",
    "    df[df['correct'] == 'Correct'],\n",
    "    df[df['correct'] == 'Incorrect'],\n",
    "])\n",
    "\n",
    "fig = px.scatter(\n",
    "    df_sorted, color='correct',\n",
    "    color_discrete_map={'Correct': '#2ecc71', 'Incorrect': '#e74c3c'},\n",
    "    category_orders={'correct': ['Correct', 'Incorrect']},\n",
    "    title='UMAP — prediction correctness',\n",
    "    **{k: v for k, v in SCATTER_KW.items() if k != 'opacity'},\n",
    "    opacity=0.55,\n",
    ")\n",
    "_style(fig).update_layout(legend_title='Prediction')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md-umap-conf",
   "metadata": {},
   "source": [
    "### 3c. Coloured by classifier confidence\n",
    "\n",
    "Confidence = max predicted probability from the logistic regression probe.  \n",
    "Low-confidence regions (blue/yellow) indicate class boundaries or ambiguous cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-umap-conf",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter(\n",
    "    df.sort_values('confidence'),   # low confidence rendered last → visible on top\n",
    "    color='confidence',\n",
    "    color_continuous_scale='RdYlGn',\n",
    "    range_color=[0, 1],\n",
    "    title='UMAP — classifier confidence (max class probability)',\n",
    "    **{k: v for k, v in SCATTER_KW.items() if k not in ('symbol', 'symbol_map')},\n",
    ")\n",
    "_style(fig)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md-umap-err",
   "metadata": {},
   "source": [
    "### 3d. Error analysis — incorrect predictions only\n",
    "\n",
    "Shows *where* in the embedding errors occur and *what* the model predicted instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-umap-err",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_err = df[df['correct'] == 'Incorrect'].copy()\n",
    "n_err  = len(df_err)\n",
    "n_tot  = len(df)\n",
    "print(f'Errors: {n_err:,} / {n_tot:,}  ({100 * n_err / n_tot:.1f} %)')\n",
    "\n",
    "fig = px.scatter(\n",
    "    df_err, color='predicted', color_discrete_map=label_colors,\n",
    "    title=f'Errors only (n={n_err:,}) — coloured by predicted class',\n",
    "    **SCATTER_KW,\n",
    ")\n",
    "_style(fig).update_layout(legend_title='Predicted as')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "meq832zmmo",
   "source": "---\n## 6. Per-class Recall Comparison\n\nCompares per-class recall across all runs listed in `COMPARE_RUNS`.\n\n- **Top panel**: grouped bar chart — one bar per run per cell type, sorted by the recall gap between the first and second run.\n- **Bottom panel**: delta bar chart (run 1 − run 2), making it easy to spot which cell types benefit most from the better architecture.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "9jie8ip1l0v",
   "source": "from pathlib import Path\n\n# ── Load per-class recall from each run's confusion_matrix_val.json ───────────\nrecalls = {}\nfor display_label, run_name in COMPARE_RUNS.items():\n    json_path = Path(RUNS_DIR) / run_name / 'confusion_matrix_val.json'\n    with open(json_path) as f:\n        data = json.load(f)\n    recalls[display_label] = data['per_class_recall']\n\nrun_labels = list(recalls.keys())\nall_classes = list(recalls[run_labels[0]].keys())\n\ndf_recall = pd.DataFrame(recalls, index=all_classes)\n\n# Sort by delta between first two runs (descending) so biggest gaps are on the right\nif len(run_labels) >= 2:\n    df_recall['_delta'] = df_recall[run_labels[0]] - df_recall[run_labels[1]]\n    df_recall = df_recall.sort_values('_delta', ascending=True)\n    delta = df_recall.pop('_delta')\nelse:\n    df_recall = df_recall.sort_values(run_labels[0], ascending=True)\n    delta = None\n\n# ── Colours ───────────────────────────────────────────────────────────────────\nPALETTE = ['#2196F3', '#FF5722', '#4CAF50', '#9C27B0', '#FF9800']\nrun_colors = {lbl: PALETTE[i] for i, lbl in enumerate(run_labels)}\n\nn_classes_cmp = len(df_recall)\nbar_width = 0.8 / len(run_labels)\nx = np.arange(n_classes_cmp)\n\n# ── Figure ────────────────────────────────────────────────────────────────────\nn_rows = 2 if delta is not None else 1\nfig, axes = plt.subplots(\n    n_rows, 1,\n    figsize=(max(12, n_classes_cmp * 0.7), 5 * n_rows),\n    gridspec_kw={'height_ratios': [3, 1]} if n_rows == 2 else {},\n)\nif n_rows == 1:\n    axes = [axes]\n\nax_bar = axes[0]\n\nfor i, lbl in enumerate(run_labels):\n    offset = (i - len(run_labels) / 2 + 0.5) * bar_width\n    bars = ax_bar.bar(\n        x + offset, df_recall[lbl],\n        width=bar_width, label=lbl,\n        color=run_colors[lbl], alpha=0.85, edgecolor='white', linewidth=0.4,\n    )\n\nax_bar.set_xticks(x)\nax_bar.set_xticklabels(df_recall.index, rotation=45, ha='right', fontsize=10)\nax_bar.set_ylabel('Recall', fontsize=11)\nax_bar.set_ylim(0, 1.05)\nax_bar.axhline(0.5, color='gray', linestyle='--', alpha=0.4, linewidth=0.8)\nax_bar.legend(fontsize=10, framealpha=0.8)\nax_bar.set_title('Per-class recall — ' + '  vs.  '.join(run_labels), fontsize=12)\nax_bar.spines[['top', 'right']].set_visible(False)\n\n# ── Delta panel ───────────────────────────────────────────────────────────────\nif delta is not None:\n    ax_delta = axes[1]\n    bar_colors = [run_colors[run_labels[0]] if v > 0 else run_colors[run_labels[1]]\n                  for v in delta.values]\n    ax_delta.bar(x, delta.values, color=bar_colors, alpha=0.85, edgecolor='white', linewidth=0.4)\n    ax_delta.axhline(0, color='black', linewidth=0.8)\n    ax_delta.set_xticks(x)\n    ax_delta.set_xticklabels(df_recall.index, rotation=45, ha='right', fontsize=10)\n    ax_delta.set_ylabel(f'Δ recall\\n({run_labels[0]} − {run_labels[1]})', fontsize=10)\n    ax_delta.set_title('Recall gap per class', fontsize=11)\n    ax_delta.spines[['top', 'right']].set_visible(False)\n\nplt.tight_layout()\nplt.show()\n\n# ── Print summary table ───────────────────────────────────────────────────────\nif delta is not None:\n    summary = df_recall[run_labels].copy()\n    summary['Δ'] = delta\n    display(summary.sort_values('Δ', ascending=False).style\n            .format('{:.3f}')\n            .background_gradient(subset=['Δ'], cmap='RdYlGn', vmin=-0.5, vmax=0.5)\n            .set_caption(f'{run_labels[0]} vs {run_labels[1]} — per-class recall'))",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "0xhry6we1ma",
   "source": "---\n## 7. Confidence–Accuracy Curve\n\nFor each confidence threshold **t**, only cells where the classifier's max predicted probability ≥ t are retained, and balanced accuracy is computed on that subset.\n\n- **Left axis (solid lines)**: balanced accuracy of retained cells — rises as uncertain cells are filtered out.\n- **Right axis (dashed lines)**: fraction of val cells retained at each threshold.\n\nA model with well-calibrated, high-quality representations should reach high accuracy while retaining a large fraction of cells.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "mf863sor2",
   "source": "from sklearn.metrics import balanced_accuracy_score\n\nTHRESHOLDS = np.linspace(0.0, 0.99, 150)\nMIN_CELLS  = 20   # skip threshold if fewer than this many cells remain\n\n# ── Load val results for every run in COMPARE_RUNS ───────────────────────────\nrun_data = {}\nfor display_label, run_name in COMPARE_RUNS.items():\n    npz = np.load(Path(RUNS_DIR) / run_name / 'val_results.npz')\n    run_data[display_label] = {\n        'labels':  npz['labels_str'],\n        'preds':   npz['top1_pred_str'],\n        'conf':    npz['logits'].max(axis=1),   # max class probability per cell\n    }\n\n# ── Compute curves ────────────────────────────────────────────────────────────\ncurves = {}   # label → dict(thresholds, accs, fractions)\n\nfor display_label, d in run_data.items():\n    labels, preds, conf = d['labels'], d['preds'], d['conf']\n    accs, fractions = [], []\n\n    for t in THRESHOLDS:\n        mask = conf >= t\n        n    = mask.sum()\n        fractions.append(n / len(conf))\n        if n >= MIN_CELLS:\n            accs.append(balanced_accuracy_score(labels[mask], preds[mask]))\n        else:\n            accs.append(np.nan)\n\n    curves[display_label] = {\n        'accs':      np.array(accs),\n        'fractions': np.array(fractions),\n    }\n\n# ── Plot ──────────────────────────────────────────────────────────────────────\nfig, ax1 = plt.subplots(figsize=(10, 5))\nax2 = ax1.twinx()\n\nfor display_label, curve in curves.items():\n    color = run_colors.get(display_label, '#333333')\n    valid = ~np.isnan(curve['accs'])\n\n    ax1.plot(\n        THRESHOLDS[valid], curve['accs'][valid],\n        color=color, linewidth=2.5, label=display_label,\n    )\n    ax2.plot(\n        THRESHOLDS, curve['fractions'],\n        color=color, linewidth=1.5, linestyle='--', alpha=0.6,\n    )\n\n# Baseline annotation at threshold=0\nfor display_label, curve in curves.items():\n    baseline = curve['accs'][0]\n    ax1.annotate(\n        f'{display_label}\\n{baseline:.3f}',\n        xy=(0.01, baseline),\n        xytext=(0.08, baseline + 0.02),\n        fontsize=8, color=run_colors.get(display_label, '#333333'),\n        arrowprops=dict(arrowstyle='->', color=run_colors.get(display_label, '#333333'), lw=1),\n    )\n\nax1.set_xlabel('Confidence threshold (min max-class probability)', fontsize=11)\nax1.set_ylabel('Balanced accuracy  (retained cells)', fontsize=11, color='black')\nax2.set_ylabel('Fraction of val cells retained', fontsize=11, color='gray')\nax1.set_ylim(0, 1.05)\nax2.set_ylim(0, 1.05)\nax2.tick_params(axis='y', colors='gray')\nax1.set_title('Confidence–accuracy curve (val set)', fontsize=12)\n\n# Combined legend: solid = accuracy, dashed = fraction retained\nfrom matplotlib.lines import Line2D\nlegend_elements = [\n    Line2D([0], [0], color=run_colors.get(lbl, '#333'), lw=2.5, label=lbl)\n    for lbl in curves\n] + [\n    Line2D([0], [0], color='gray', lw=1.5, linestyle='--', label='Fraction retained (dashed)')\n]\nax1.legend(handles=legend_elements, loc='lower right', fontsize=9, framealpha=0.85)\nax1.spines[['top']].set_visible(False)\n\nplt.tight_layout()\nplt.show()\n\n# ── Summary: accuracy at key thresholds ──────────────────────────────────────\nkey_thresholds = [0.0, 0.5, 0.7, 0.9, 0.95]\nrows = []\nfor t in key_thresholds:\n    idx = np.argmin(np.abs(THRESHOLDS - t))\n    row = {'Threshold': f'{t:.2f}'}\n    for display_label, curve in curves.items():\n        acc  = curve['accs'][idx]\n        frac = curve['fractions'][idx]\n        row[f'{display_label} acc']      = f'{acc:.3f}'  if not np.isnan(acc) else '–'\n        row[f'{display_label} retained'] = f'{frac:.1%}'\n    rows.append(row)\n\ndisplay(pd.DataFrame(rows).set_index('Threshold').style.set_caption(\n    'Accuracy and fraction retained at key confidence thresholds'\n))",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "md-markers",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Marker Activations in UMAP Space\n",
    "\n",
    "The feature vector is structured as `(C × FEAT_PER_CH,)` where `C` is the number of markers.  \n",
    "We take the mean over the `FEAT_PER_CH` channels per marker to get a scalar activation per cell per marker,\n",
    "then overlay this on the UMAP.\n",
    "\n",
    "**Requires** the dataset config to resolve marker names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-dataset",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mmengine import Config\n",
    "from mmengine.registry import DATASETS\n",
    "\n",
    "cfg_path = list(RUN_DIR.glob('*.py'))[0]\n",
    "cfg      = Config.fromfile(str(cfg_path))\n",
    "dataset  = DATASETS.build(cfg['train_dataset'])\n",
    "\n",
    "markers = list(dataset.marker2idx.keys())\n",
    "print(f'Markers ({len(markers)}): {markers}')\n",
    "\n",
    "expected_feat_dim = len(markers) * FEAT_PER_CH\n",
    "actual_feat_dim   = all_feat.shape[1]\n",
    "if expected_feat_dim != actual_feat_dim:\n",
    "    print(f'\\nWARNING: expected {expected_feat_dim} = {len(markers)} × {FEAT_PER_CH}'\n",
    "          f' but features have dim {actual_feat_dim}.')\n",
    "    print('Adjust FEAT_PER_CH in the Configuration cell.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-activations",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (N, C*F) → mean over F → (N, C)\n",
    "activations = einops.rearrange(\n",
    "    all_feat, 'N (C F) -> N C F', C=len(markers), F=FEAT_PER_CH\n",
    ").mean(axis=-1)\n",
    "\n",
    "for marker, idx in dataset.marker2idx.items():\n",
    "    df[marker] = activations[:, idx]\n",
    "\n",
    "print(f'Added {len(markers)} marker activation columns to the DataFrame.')\n",
    "display(df[markers].describe().round(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md-marker-grid",
   "metadata": {},
   "source": [
    "### Marker activation grid\n",
    "\n",
    "Each sub-plot shows the UMAP coloured by one marker's activation.  \n",
    "Colour is clipped to the 2–98th percentile range to suppress outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-marker-grid",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_COLS = 4\n",
    "N_ROWS = int(np.ceil(len(markers) / N_COLS))\n",
    "\n",
    "fig = make_subplots(\n",
    "    rows=N_ROWS, cols=N_COLS,\n",
    "    subplot_titles=markers,\n",
    "    shared_xaxes=True, shared_yaxes=True,\n",
    "    vertical_spacing=0.04, horizontal_spacing=0.02,\n",
    ")\n",
    "\n",
    "for i, marker in enumerate(markers):\n",
    "    row, col = divmod(i, N_COLS)\n",
    "    vals = df[marker].values\n",
    "    vmin, vmax = np.percentile(vals, [2, 98])\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Scattergl(\n",
    "            x=df['x'], y=df['y'],\n",
    "            mode='markers',\n",
    "            marker=dict(\n",
    "                size=2,\n",
    "                color=vals,\n",
    "                colorscale='Viridis',\n",
    "                cmin=vmin, cmax=vmax,\n",
    "                showscale=(i == 0),\n",
    "                colorbar=dict(title='activation', thickness=10, len=0.3, y=0.85) if i == 0 else {},\n",
    "            ),\n",
    "            showlegend=False,\n",
    "            hovertemplate=f'<b>{marker}</b>: %{{marker.color:.3f}}<br>label: %{{text}}<extra></extra>',\n",
    "            text=df['label'],\n",
    "        ),\n",
    "        row=row + 1, col=col + 1,\n",
    "    )\n",
    "\n",
    "fig.update_xaxes(showticklabels=False, showgrid=False, zeroline=False)\n",
    "fig.update_yaxes(showticklabels=False, showgrid=False, zeroline=False)\n",
    "fig.update_layout(\n",
    "    title='Marker activations on UMAP',\n",
    "    height=260 * N_ROWS,\n",
    "    width=1100,\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md-single-marker",
   "metadata": {},
   "source": [
    "### Single-marker interactive view\n",
    "\n",
    "Change `MARKER` to inspect any one marker in detail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-single-marker",
   "metadata": {},
   "outputs": [],
   "source": [
    "MARKER = markers[0]  # ← change this\n",
    "\n",
    "vals = df[MARKER].values\n",
    "vmin, vmax = np.percentile(vals, [2, 98])\n",
    "\n",
    "fig = px.scatter(\n",
    "    df, x='x', y='y',\n",
    "    color=MARKER,\n",
    "    color_continuous_scale='Viridis',\n",
    "    range_color=[vmin, vmax],\n",
    "    hover_data=['label', 'predicted', MARKER],\n",
    "    title=f'Marker activation: {MARKER}',\n",
    "    width=900, height=650, opacity=0.7,\n",
    ")\n",
    "fig.update_traces(marker=dict(size=4, line=dict(width=0)))\n",
    "fig.update_layout(xaxis_title='UMAP-1', yaxis_title='UMAP-2')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md-profile",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Per-class Marker Profile\n",
    "\n",
    "Shows which markers are most active (or uniquely active) for each cell type.\n",
    "\n",
    "- **Raw**: mean activation per (class, marker) pair.\n",
    "- **Z-scored**: each marker's activations are standardised across classes,\n",
    "  so a high value means that class has unusually high activation for that marker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-profile-heatmap",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import zscore\n",
    "\n",
    "class_means = (\n",
    "    df.groupby('label')[markers]\n",
    "    .mean()\n",
    "    .T   # markers as rows, classes as columns\n",
    ")\n",
    "class_z = class_means.apply(zscore, axis=1)  # z-score across classes per marker\n",
    "\n",
    "cell_h = max(0.35, 6 / len(markers))\n",
    "cell_w = max(0.7,  6 / n_classes)\n",
    "\n",
    "fig, axes = plt.subplots(\n",
    "    1, 2,\n",
    "    figsize=(cell_w * n_classes * 2 + 3, cell_h * len(markers) + 1),\n",
    ")\n",
    "\n",
    "for ax, data, title, cmap, center in [\n",
    "    (axes[0], class_means, 'Mean activation (raw)',                  'YlOrRd', None),\n",
    "    (axes[1], class_z,     'Z-scored activation (across classes)',   'RdBu_r', 0.0),\n",
    "]:\n",
    "    vabs = np.abs(data.values).max() if center is not None else None\n",
    "    vmin = -vabs if center is not None else data.values.min()\n",
    "    vmax =  vabs if center is not None else data.values.max()\n",
    "\n",
    "    im = ax.imshow(data.values, aspect='auto', cmap=cmap, vmin=vmin, vmax=vmax)\n",
    "    plt.colorbar(im, ax=ax, shrink=0.6)\n",
    "\n",
    "    ax.set_xticks(range(len(data.columns)))\n",
    "    ax.set_xticklabels(data.columns, rotation=45, ha='right', fontsize=9)\n",
    "    ax.set_yticks(range(len(data.index)))\n",
    "    ax.set_yticklabels(data.index, fontsize=9)\n",
    "    ax.set_xlabel('Cell type')\n",
    "    ax.set_ylabel('Marker')\n",
    "    ax.set_title(title, fontsize=11)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md-top-markers",
   "metadata": {},
   "source": [
    "### Top discriminative markers per class\n",
    "\n",
    "Ranked by z-scored activation — the markers that are most uniquely high for each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-top-markers",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_TOP = 5\n",
    "\n",
    "rows = []\n",
    "for cls in sorted(class_z.columns):\n",
    "    top = class_z[cls].nlargest(N_TOP)\n",
    "    rows.append({'Class': cls, **{f'#{i+1}': f'{m} ({v:.2f})' for i, (m, v) in enumerate(top.items())}})\n",
    "\n",
    "display(pd.DataFrame(rows).set_index('Class').style.set_caption(\n",
    "    f'Top {N_TOP} markers per class (z-scored activation)'\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md-marker-bars",
   "metadata": {},
   "source": [
    "### Per-marker expression boxplot by class\n",
    "\n",
    "Change `MARKER` to inspect any marker's distribution across all classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ebb490e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-marker-box",
   "metadata": {},
   "outputs": [],
   "source": [
    "MARKER = 'CD4' #markers[10]  # ← change this\n",
    "\n",
    "fig = px.box(\n",
    "    df, x='label', y=MARKER,\n",
    "    color='label', color_discrete_map=label_colors,\n",
    "    points='outliers',\n",
    "    title=f'Activation distribution: {MARKER}',\n",
    "    labels={'label': 'Cell type', MARKER: 'Activation'},\n",
    "    width=900, height=500,\n",
    ")\n",
    "fig.update_layout(showlegend=False, xaxis_tickangle=-35)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md-footer",
   "metadata": {},
   "source": [
    "---\n",
    "> **Next steps**  \n",
    "> - For spatial attention masks and channel cross-attention maps, see **`attention_maps.ipynb`**.  \n",
    "> - For raw dataset exploration (patch browsing, marker coverage), see **`dataset_exploration.ipynb`**."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}