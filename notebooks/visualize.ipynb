{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "md-title",
   "metadata": {},
   "source": [
    "# MCA Experiment Visualisation\n",
    "\n",
    "End-to-end analysis of a training run from the `.npz` result files.\n",
    "\n",
    "| Section | What it shows |\n",
    "|---|---|\n",
    "| **1. Load Results** | Feature shapes, class distribution |\n",
    "| **2. Classification Metrics** | Accuracy table + confusion matrix |\n",
    "| **3. UMAP Embedding** | 2-D projection coloured by label / prediction / confidence |\n",
    "| **4. Marker Activations** | Per-marker feature means overlaid on UMAP |\n",
    "| **5. Per-class Marker Profile** | Heatmap of which markers drive each cell-type |\n",
    "\n",
    "> **Model attention maps** are covered in `attention_maps.ipynb`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md-cfg",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "Edit these variables before running the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-cfg",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Edit these ────────────────────────────────────────────────────────────────\n",
    "DATASET_NAME = 'CODEX_cHL_CIM_MASK_VP_LONG'\n",
    "RUNS_DIR     = '/home/simon_g/isilon_images_mnt/10_MetaSystems/MetaSystemsData/_simon/src/MCA/z_RUNS'\n",
    "\n",
    "N_TRAIN      = 10_000   # train samples to subsample for UMAP\n",
    "N_VAL        =  5_000   # val   samples to subsample for UMAP\n",
    "UMAP_DIMS    = 2        # 2 or 3\n",
    "FEAT_PER_CH  = 32       # feature channels per marker (must match model config)\n",
    "RANDOM_SEED  = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md-imports",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "import einops\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import umap\n",
    "from IPython.display import display\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "RUN_DIR = Path(RUNS_DIR) / DATASET_NAME\n",
    "print(f'Run directory: {RUN_DIR}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md-load",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Load Results\n",
    "\n",
    "The `.npz` files are written by `val_hook.py` after training.\n",
    "Each file contains features, string labels, top-1 / top-2 predictions, per-class logits, and sample IDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-load",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file = np.load(RUN_DIR / 'train_results.npz')\n",
    "val_file   = np.load(RUN_DIR / 'val_results.npz')\n",
    "\n",
    "train_features   = train_file['features']\n",
    "val_features     = val_file['features']\n",
    "train_labels_str = train_file['labels_str']\n",
    "val_labels_str   = val_file['labels_str']\n",
    "train_preds_str  = train_file['top1_pred_str']\n",
    "val_preds_str    = val_file['top1_pred_str']\n",
    "train_logits     = train_file['logits']   # (N, n_classes) – classifier probabilities\n",
    "val_logits       = val_file['logits']\n",
    "\n",
    "classes   = list(val_file['classes'])\n",
    "n_classes = len(classes)\n",
    "\n",
    "print(f'Dataset : {DATASET_NAME}')\n",
    "print(f'Classes : {classes}')\n",
    "print()\n",
    "print(f'Train  → {len(train_features):>7,} cells  |  feature dim: {train_features.shape[1]}')\n",
    "print(f'Val    → {len(val_features):>7,} cells  |  feature dim: {val_features.shape[1]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md-dist",
   "metadata": {},
   "source": [
    "### Class distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-dist",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(14, 4), sharey=False)\n",
    "\n",
    "for ax, labels, split in [\n",
    "    (axes[0], train_labels_str, 'Train'),\n",
    "    (axes[1], val_labels_str,   'Val'),\n",
    "]:\n",
    "    unique, counts = np.unique(labels, return_counts=True)\n",
    "    order = np.argsort(counts)[::-1]\n",
    "    bars = ax.bar(unique[order], counts[order], color='steelblue', edgecolor='white', linewidth=0.5)\n",
    "\n",
    "    # Label each bar with its count\n",
    "    for bar, cnt in zip(bars, counts[order]):\n",
    "        ax.text(\n",
    "            bar.get_x() + bar.get_width() / 2,\n",
    "            bar.get_height() + counts.max() * 0.01,\n",
    "            f'{cnt:,}', ha='center', va='bottom', fontsize=8\n",
    "        )\n",
    "\n",
    "    ax.set_title(f'{split} class distribution  (n={len(labels):,})', fontsize=11)\n",
    "    ax.set_xlabel('Cell type')\n",
    "    ax.set_ylabel('Count')\n",
    "    ax.tick_params(axis='x', rotation=40)\n",
    "    ax.spines[['top', 'right']].set_visible(False)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md-metrics",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Classification Metrics\n",
    "\n",
    "Metrics are computed by a logistic regression probe trained on the frozen features (see `val_hook.py`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-metrics-table",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_path = RUN_DIR / 'metrics.json'\n",
    "\n",
    "if metrics_path.exists():\n",
    "    with open(metrics_path) as f:\n",
    "        metrics = json.load(f)\n",
    "\n",
    "    rows = []\n",
    "    for split in ['train', 'val']:\n",
    "        m = metrics[split]\n",
    "        rows.append({\n",
    "            'Split':             split.capitalize(),\n",
    "            'Top-1 Acc':         f\"{m['top1_accuracy']:.3f}\",\n",
    "            'Top-2 Acc':         f\"{m['top2_accuracy']:.3f}\",\n",
    "            'Bal. Acc (top-1)':  f\"{m['top1_balanced_accuracy']:.3f}\",\n",
    "            'Bal. Acc (top-2)':  f\"{m['top2_balanced_accuracy']:.3f}\",\n",
    "            'F1 (weighted)':     f\"{m['f1']:.3f}\",\n",
    "            'N samples':         f\"{m['n_samples']:,}\",\n",
    "        })\n",
    "\n",
    "    display(pd.DataFrame(rows).set_index('Split').style.set_caption(\n",
    "        f'Logistic-regression probe — {n_classes} classes'\n",
    "    ))\n",
    "else:\n",
    "    print(f'metrics.json not found at {metrics_path}\\nCompute metrics inline:')\n",
    "    from sklearn.metrics import accuracy_score, balanced_accuracy_score, f1_score\n",
    "    for labels, preds, split in [\n",
    "        (train_labels_str, train_preds_str, 'Train'),\n",
    "        (val_labels_str,   val_preds_str,   'Val'),\n",
    "    ]:\n",
    "        acc  = accuracy_score(labels, preds)\n",
    "        bacc = balanced_accuracy_score(labels, preds)\n",
    "        f1   = f1_score(labels, preds, average='weighted')\n",
    "        print(f'  {split}: Acc={acc:.3f}  Bal.Acc={bacc:.3f}  F1={f1:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md-cm",
   "metadata": {},
   "source": [
    "### Confusion matrices\n",
    "\n",
    "Row-normalised (true label on y-axis, predicted on x-axis).  \n",
    "The diagonal is the per-class recall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-cm",
   "metadata": {},
   "outputs": [],
   "source": [
    "cell_size = max(0.7, 6 / n_classes)\n",
    "fig, axes = plt.subplots(1, 2, figsize=(cell_size * n_classes * 2 + 2, cell_size * n_classes + 1))\n",
    "\n",
    "for ax, labels, preds, title in [\n",
    "    (axes[0], train_labels_str, train_preds_str, 'Train'),\n",
    "    (axes[1], val_labels_str,   val_preds_str,   'Val'),\n",
    "]:\n",
    "    cm = confusion_matrix(labels, preds, labels=classes, normalize='true')\n",
    "    im = ax.imshow(cm, cmap='Blues', vmin=0, vmax=1)\n",
    "\n",
    "    for i in range(n_classes):\n",
    "        for j in range(n_classes):\n",
    "            ax.text(j, i, f'{cm[i, j]:.2f}',\n",
    "                    ha='center', va='center', fontsize=8,\n",
    "                    color='white' if cm[i, j] > 0.55 else 'black')\n",
    "\n",
    "    ax.set_xticks(range(n_classes))\n",
    "    ax.set_yticks(range(n_classes))\n",
    "    ax.set_xticklabels(classes, rotation=45, ha='right', fontsize=9)\n",
    "    ax.set_yticklabels(classes, fontsize=9)\n",
    "    ax.set_xlabel('Predicted')\n",
    "    ax.set_ylabel('True')\n",
    "    ax.set_title(f'{title} confusion matrix (row-normalised)', fontsize=11)\n",
    "    plt.colorbar(im, ax=ax, shrink=0.8)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md-umap",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. UMAP Embedding\n",
    "\n",
    "UMAP projects the high-dimensional features into 2-D for visual inspection.\n",
    "We subsample from the full split to keep the embedding tractable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-subsample",
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.default_rng(RANDOM_SEED)\n",
    "\n",
    "def _subsample(feat, labels, preds, logits, n):\n",
    "    idx = rng.permutation(len(feat))[:n]\n",
    "    return feat[idx], labels[idx], preds[idx], logits[idx]\n",
    "\n",
    "tr_feat, tr_lab, tr_pred, tr_logit = _subsample(\n",
    "    train_features, train_labels_str, train_preds_str, train_logits, N_TRAIN\n",
    ")\n",
    "va_feat, va_lab, va_pred, va_logit = _subsample(\n",
    "    val_features, val_labels_str, val_preds_str, val_logits, N_VAL\n",
    ")\n",
    "\n",
    "all_feat  = np.vstack([tr_feat, va_feat])\n",
    "all_lab   = np.concatenate([tr_lab,   va_lab])\n",
    "all_pred  = np.concatenate([tr_pred,  va_pred])\n",
    "all_split = np.array(['Train'] * len(tr_feat) + ['Val'] * len(va_feat))\n",
    "all_conf  = np.concatenate([tr_logit.max(1), va_logit.max(1)])  # top-class probability\n",
    "all_corr  = np.where(all_lab == all_pred, 'Correct', 'Incorrect')\n",
    "\n",
    "print(f'Subsampled: {len(tr_feat):,} train + {len(va_feat):,} val = {len(all_feat):,} total')\n",
    "print(f'Subsample accuracy: {(all_lab == all_pred).mean():.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-umap-fit",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Fitting UMAP…')\n",
    "reducer = umap.UMAP(\n",
    "    n_neighbors=15,\n",
    "    min_dist=0.05,\n",
    "    n_components=UMAP_DIMS,\n",
    "    metric='euclidean',\n",
    "    random_state=RANDOM_SEED,\n",
    "    n_jobs=4,\n",
    "    verbose=True,\n",
    ")\n",
    "embedding = reducer.fit_transform(all_feat)\n",
    "print(f'Done. Embedding shape: {embedding.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-umap-df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a single DataFrame used by all downstream plots\n",
    "df = pd.DataFrame({\n",
    "    'x':          embedding[:, 0],\n",
    "    'y':          embedding[:, 1],\n",
    "    'label':      all_lab,\n",
    "    'predicted':  all_pred,\n",
    "    'correct':    all_corr,\n",
    "    'confidence': all_conf,\n",
    "    'split':      all_split,\n",
    "})\n",
    "if UMAP_DIMS == 3:\n",
    "    df['z'] = embedding[:, 2]\n",
    "\n",
    "# Consistent colour palette (same colour = same class, across all plots)\n",
    "label_colors = {\n",
    "    lab: px.colors.qualitative.Alphabet[i % len(px.colors.qualitative.Alphabet)]\n",
    "    for i, lab in enumerate(sorted(df['label'].unique()))\n",
    "}\n",
    "\n",
    "SCATTER_KW = dict(  # shared keyword arguments for scatter plots\n",
    "    x='x', y='y',\n",
    "    symbol='split', symbol_map={'Train': 'circle', 'Val': 'cross'},\n",
    "    hover_data=['label', 'predicted', 'correct', 'confidence', 'split'],\n",
    "    width=1100, height=700,\n",
    "    opacity=0.65,\n",
    ")\n",
    "\n",
    "def _style(fig):\n",
    "    fig.update_traces(marker=dict(size=4, line=dict(width=0)))\n",
    "    fig.update_layout(xaxis_title='UMAP-1', yaxis_title='UMAP-2')\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md-umap-plots",
   "metadata": {},
   "source": [
    "### 3a. Coloured by true cell type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-umap-label",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter(\n",
    "    df, color='label', color_discrete_map=label_colors,\n",
    "    title='UMAP — true cell type',\n",
    "    **SCATTER_KW\n",
    ")\n",
    "_style(fig).update_layout(legend_title='Cell type')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md-umap-corr",
   "metadata": {},
   "source": [
    "### 3b. Correct vs. incorrect predictions\n",
    "\n",
    "Green points are correctly classified; red points are errors.  \n",
    "Clusters of red points suggest confusion between specific cell types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-umap-correct",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put Correct on top so Incorrect (red) is not hidden\n",
    "df_sorted = pd.concat([\n",
    "    df[df['correct'] == 'Correct'],\n",
    "    df[df['correct'] == 'Incorrect'],\n",
    "])\n",
    "\n",
    "fig = px.scatter(\n",
    "    df_sorted, color='correct',\n",
    "    color_discrete_map={'Correct': '#2ecc71', 'Incorrect': '#e74c3c'},\n",
    "    category_orders={'correct': ['Correct', 'Incorrect']},\n",
    "    title='UMAP — prediction correctness',\n",
    "    **{k: v for k, v in SCATTER_KW.items() if k != 'opacity'},\n",
    "    opacity=0.55,\n",
    ")\n",
    "_style(fig).update_layout(legend_title='Prediction')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md-umap-conf",
   "metadata": {},
   "source": [
    "### 3c. Coloured by classifier confidence\n",
    "\n",
    "Confidence = max predicted probability from the logistic regression probe.  \n",
    "Low-confidence regions (blue/yellow) indicate class boundaries or ambiguous cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-umap-conf",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter(\n",
    "    df.sort_values('confidence'),   # low confidence rendered last → visible on top\n",
    "    color='confidence',\n",
    "    color_continuous_scale='RdYlGn',\n",
    "    range_color=[0, 1],\n",
    "    title='UMAP — classifier confidence (max class probability)',\n",
    "    **{k: v for k, v in SCATTER_KW.items() if k not in ('symbol', 'symbol_map')},\n",
    ")\n",
    "_style(fig)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md-umap-err",
   "metadata": {},
   "source": [
    "### 3d. Error analysis — incorrect predictions only\n",
    "\n",
    "Shows *where* in the embedding errors occur and *what* the model predicted instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-umap-err",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_err = df[df['correct'] == 'Incorrect'].copy()\n",
    "n_err  = len(df_err)\n",
    "n_tot  = len(df)\n",
    "print(f'Errors: {n_err:,} / {n_tot:,}  ({100 * n_err / n_tot:.1f} %)')\n",
    "\n",
    "fig = px.scatter(\n",
    "    df_err, color='predicted', color_discrete_map=label_colors,\n",
    "    title=f'Errors only (n={n_err:,}) — coloured by predicted class',\n",
    "    **SCATTER_KW,\n",
    ")\n",
    "_style(fig).update_layout(legend_title='Predicted as')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md-markers",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Marker Activations in UMAP Space\n",
    "\n",
    "The feature vector is structured as `(C × FEAT_PER_CH,)` where `C` is the number of markers.  \n",
    "We take the mean over the `FEAT_PER_CH` channels per marker to get a scalar activation per cell per marker,\n",
    "then overlay this on the UMAP.\n",
    "\n",
    "**Requires** the dataset config to resolve marker names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-dataset",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mmengine import Config\n",
    "from mmengine.registry import DATASETS\n",
    "\n",
    "cfg_path = RUN_DIR / f'{DATASET_NAME}.py'\n",
    "cfg      = Config.fromfile(str(cfg_path))\n",
    "dataset  = DATASETS.build(cfg['train_dataset'])\n",
    "\n",
    "markers = list(dataset.marker2idx.keys())\n",
    "print(f'Markers ({len(markers)}): {markers}')\n",
    "\n",
    "expected_feat_dim = len(markers) * FEAT_PER_CH\n",
    "actual_feat_dim   = all_feat.shape[1]\n",
    "if expected_feat_dim != actual_feat_dim:\n",
    "    print(f'\\nWARNING: expected {expected_feat_dim} = {len(markers)} × {FEAT_PER_CH}'\n",
    "          f' but features have dim {actual_feat_dim}.')\n",
    "    print('Adjust FEAT_PER_CH in the Configuration cell.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-activations",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (N, C*F) → mean over F → (N, C)\n",
    "activations = einops.rearrange(\n",
    "    all_feat, 'N (C F) -> N C F', C=len(markers), F=FEAT_PER_CH\n",
    ").mean(axis=-1)\n",
    "\n",
    "for marker, idx in dataset.marker2idx.items():\n",
    "    df[marker] = activations[:, idx]\n",
    "\n",
    "print(f'Added {len(markers)} marker activation columns to the DataFrame.')\n",
    "display(df[markers].describe().round(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md-marker-grid",
   "metadata": {},
   "source": [
    "### Marker activation grid\n",
    "\n",
    "Each sub-plot shows the UMAP coloured by one marker's activation.  \n",
    "Colour is clipped to the 2–98th percentile range to suppress outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-marker-grid",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_COLS = 4\n",
    "N_ROWS = int(np.ceil(len(markers) / N_COLS))\n",
    "\n",
    "fig = make_subplots(\n",
    "    rows=N_ROWS, cols=N_COLS,\n",
    "    subplot_titles=markers,\n",
    "    shared_xaxes=True, shared_yaxes=True,\n",
    "    vertical_spacing=0.04, horizontal_spacing=0.02,\n",
    ")\n",
    "\n",
    "for i, marker in enumerate(markers):\n",
    "    row, col = divmod(i, N_COLS)\n",
    "    vals = df[marker].values\n",
    "    vmin, vmax = np.percentile(vals, [2, 98])\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Scattergl(\n",
    "            x=df['x'], y=df['y'],\n",
    "            mode='markers',\n",
    "            marker=dict(\n",
    "                size=2,\n",
    "                color=vals,\n",
    "                colorscale='Viridis',\n",
    "                cmin=vmin, cmax=vmax,\n",
    "                showscale=(i == 0),\n",
    "                colorbar=dict(title='activation', thickness=10, len=0.3, y=0.85) if i == 0 else {},\n",
    "            ),\n",
    "            showlegend=False,\n",
    "            hovertemplate=f'<b>{marker}</b>: %{{marker.color:.3f}}<br>label: %{{text}}<extra></extra>',\n",
    "            text=df['label'],\n",
    "        ),\n",
    "        row=row + 1, col=col + 1,\n",
    "    )\n",
    "\n",
    "fig.update_xaxes(showticklabels=False, showgrid=False, zeroline=False)\n",
    "fig.update_yaxes(showticklabels=False, showgrid=False, zeroline=False)\n",
    "fig.update_layout(\n",
    "    title='Marker activations on UMAP',\n",
    "    height=260 * N_ROWS,\n",
    "    width=1100,\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md-single-marker",
   "metadata": {},
   "source": [
    "### Single-marker interactive view\n",
    "\n",
    "Change `MARKER` to inspect any one marker in detail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-single-marker",
   "metadata": {},
   "outputs": [],
   "source": [
    "MARKER = markers[0]  # ← change this\n",
    "\n",
    "vals = df[MARKER].values\n",
    "vmin, vmax = np.percentile(vals, [2, 98])\n",
    "\n",
    "fig = px.scatter(\n",
    "    df, x='x', y='y',\n",
    "    color=MARKER,\n",
    "    color_continuous_scale='Viridis',\n",
    "    range_color=[vmin, vmax],\n",
    "    hover_data=['label', 'predicted', MARKER],\n",
    "    title=f'Marker activation: {MARKER}',\n",
    "    width=900, height=650, opacity=0.7,\n",
    ")\n",
    "fig.update_traces(marker=dict(size=4, line=dict(width=0)))\n",
    "fig.update_layout(xaxis_title='UMAP-1', yaxis_title='UMAP-2')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md-profile",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Per-class Marker Profile\n",
    "\n",
    "Shows which markers are most active (or uniquely active) for each cell type.\n",
    "\n",
    "- **Raw**: mean activation per (class, marker) pair.\n",
    "- **Z-scored**: each marker's activations are standardised across classes,\n",
    "  so a high value means that class has unusually high activation for that marker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-profile-heatmap",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import zscore\n",
    "\n",
    "class_means = (\n",
    "    df.groupby('label')[markers]\n",
    "    .mean()\n",
    "    .T   # markers as rows, classes as columns\n",
    ")\n",
    "class_z = class_means.apply(zscore, axis=1)  # z-score across classes per marker\n",
    "\n",
    "cell_h = max(0.35, 6 / len(markers))\n",
    "cell_w = max(0.7,  6 / n_classes)\n",
    "\n",
    "fig, axes = plt.subplots(\n",
    "    1, 2,\n",
    "    figsize=(cell_w * n_classes * 2 + 3, cell_h * len(markers) + 1),\n",
    ")\n",
    "\n",
    "for ax, data, title, cmap, center in [\n",
    "    (axes[0], class_means, 'Mean activation (raw)',                  'YlOrRd', None),\n",
    "    (axes[1], class_z,     'Z-scored activation (across classes)',   'RdBu_r', 0.0),\n",
    "]:\n",
    "    vabs = np.abs(data.values).max() if center is not None else None\n",
    "    vmin = -vabs if center is not None else data.values.min()\n",
    "    vmax =  vabs if center is not None else data.values.max()\n",
    "\n",
    "    im = ax.imshow(data.values, aspect='auto', cmap=cmap, vmin=vmin, vmax=vmax)\n",
    "    plt.colorbar(im, ax=ax, shrink=0.6)\n",
    "\n",
    "    ax.set_xticks(range(len(data.columns)))\n",
    "    ax.set_xticklabels(data.columns, rotation=45, ha='right', fontsize=9)\n",
    "    ax.set_yticks(range(len(data.index)))\n",
    "    ax.set_yticklabels(data.index, fontsize=9)\n",
    "    ax.set_xlabel('Cell type')\n",
    "    ax.set_ylabel('Marker')\n",
    "    ax.set_title(title, fontsize=11)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md-top-markers",
   "metadata": {},
   "source": [
    "### Top discriminative markers per class\n",
    "\n",
    "Ranked by z-scored activation — the markers that are most uniquely high for each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-top-markers",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_TOP = 5\n",
    "\n",
    "rows = []\n",
    "for cls in sorted(class_z.columns):\n",
    "    top = class_z[cls].nlargest(N_TOP)\n",
    "    rows.append({'Class': cls, **{f'#{i+1}': f'{m} ({v:.2f})' for i, (m, v) in enumerate(top.items())}})\n",
    "\n",
    "display(pd.DataFrame(rows).set_index('Class').style.set_caption(\n",
    "    f'Top {N_TOP} markers per class (z-scored activation)'\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md-marker-bars",
   "metadata": {},
   "source": [
    "### Per-marker expression boxplot by class\n",
    "\n",
    "Change `MARKER` to inspect any marker's distribution across all classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-marker-box",
   "metadata": {},
   "outputs": [],
   "source": [
    "MARKER = markers[0]  # ← change this\n",
    "\n",
    "fig = px.box(\n",
    "    df, x='label', y=MARKER,\n",
    "    color='label', color_discrete_map=label_colors,\n",
    "    points='outliers',\n",
    "    title=f'Activation distribution: {MARKER}',\n",
    "    labels={'label': 'Cell type', MARKER: 'Activation'},\n",
    "    width=900, height=500,\n",
    ")\n",
    "fig.update_layout(showlegend=False, xaxis_tickangle=-35)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md-footer",
   "metadata": {},
   "source": [
    "---\n",
    "> **Next steps**  \n",
    "> - For spatial attention masks and channel cross-attention maps, see **`attention_maps.ipynb`**.  \n",
    "> - For raw dataset exploration (patch browsing, marker coverage), see **`dataset_exploration.ipynb`**."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
