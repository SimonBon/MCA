{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Architecture Comparison\n",
    "\n",
    "This notebook tests all implemented backbone architectures for multiplex spatial proteomics cell phenotyping.\n",
    "\n",
    "The central research question is: **is channel separability a better inductive bias than early fusion?**\n",
    "\n",
    "Each cell below instantiates one model, runs a forward pass with dummy data, and reports parameter count and output shape.\n",
    "\n",
    "**Test setup:** `n_channels=41` (cHL panel), `input_size=24` (cutter_size), `batch_size=4`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from src.models import WideModel, SharedStemModel, WideModelAttention\n",
    "from src.models_attention import MCIANet\n",
    "from src.models_early_fusion import EarlyFusionModel, MidFusionModel, ProjectionFusionModel, ResNetBaseline\n",
    "\n",
    "# Shared test inputs\n",
    "N_CHANNELS  = 41\n",
    "INPUT_SIZE  = 24\n",
    "BATCH_SIZE  = 4\n",
    "STEM_WIDTH  = 32   # features_per_marker\n",
    "\n",
    "x = torch.randn(BATCH_SIZE, N_CHANNELS, INPUT_SIZE, INPUT_SIZE)\n",
    "\n",
    "def summarise(name, model, x):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        out = model(x)\n",
    "    out_tensor = out[0]\n",
    "    n_params = sum(p.numel() for p in model.parameters())\n",
    "    print(f\"{'Model':<30} {'Output shape':<25} {'Params':>10}\")\n",
    "    print(f\"{name:<30} {str(tuple(out_tensor.shape)):<25} {n_params:>10,}\")\n",
    "    return out_tensor\n",
    "\n",
    "print('Setup complete. Input shape:', x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Channel-Separable Architectures\n",
    "These models maintain strict per-channel independence throughout the convolutional stem and processing layers. Cross-channel mixing only occurs at the end (late fusion / attention)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. WideModel (CIM — Channel-Independent Model)\n",
    "\n",
    "**Key design:** Depthwise grouped convolutions (`groups=in_channels`) throughout stem and all ConvBlocks. Each marker channel has its own independent set of convolutional filters. Channels are **never mixed** during spatial feature extraction — the model treats each marker as a completely independent spatial signal.\n",
    "\n",
    "Optional `late_fusion=True` adds a single 1×1 conv at the end to allow cross-channel mixing after global pooling.\n",
    "\n",
    "**Inductive bias:** Spatial patterns within each marker channel are independently meaningful. Cell identity emerges from aggregating per-marker features, not from pixel-level cross-marker interactions.\n",
    "\n",
    "**This is the proposed architecture / positive hypothesis.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_cim = WideModel(\n",
    "    in_channels=N_CHANNELS,\n",
    "    stem_width=STEM_WIDTH,\n",
    "    block_width=2,\n",
    "    layer_config=[1, 1],\n",
    "    late_fusion=False,\n",
    "    drop_prob=0.05,\n",
    ")\n",
    "summarise('WideModel (CIM)', model_cim, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. SharedStemModel (SSM — Shared Stem Model)\n",
    "\n",
    "**Key design:** A single shared convolutional stem (weights shared across all channels) is applied independently to each marker channel. Unlike WideModel which has separate weights per channel, here all channels share the same filter — the stem learns a universal single-channel spatial feature extractor.\n",
    "\n",
    "**Inductive bias:** All marker channels share the same spatial statistics (e.g., punctate vs diffuse staining). Marker identity is encoded only via the channel position after concatenation.\n",
    "\n",
    "**Compared to WideModel:** SSM has dramatically fewer parameters in the stem (1 filter set vs C filter sets). This is more parameter-efficient but loses per-marker specialisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ssm = SharedStemModel(\n",
    "    in_channels=N_CHANNELS,\n",
    "    stem_width=STEM_WIDTH,\n",
    "    block_width=2,\n",
    "    n_layers=2,\n",
    "    late_fusion=False,\n",
    ")\n",
    "summarise('SharedStemModel (SSM)', model_ssm, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. WideModelAttention (CIMATT — CIM with Attention)\n",
    "\n",
    "**Key design:** Channel-separable stem and ConvBlocks (identical to WideModel), followed by **multi-head self-attention across channel tokens**. After spatial processing, each channel contributes one token (spatially pooled). Attention computes cross-channel relationships and gates the spatial feature maps before final pooling.\n",
    "\n",
    "**Inductive bias:** Same as WideModel for spatial feature extraction, but adds an explicit mechanism to learn which marker co-expression patterns matter (e.g., CD4 and CD25 together → TReg signal).\n",
    "\n",
    "**Compared to WideModel:** The attention layer is the only place where channel interactions occur. This is a principled, interpretable form of late fusion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_cimatt = WideModelAttention(\n",
    "    in_channels=N_CHANNELS,\n",
    "    stem_width=STEM_WIDTH,\n",
    "    block_width=2,\n",
    "    layer_config=[1, 1],\n",
    "    drop_prob=0.05,\n",
    "    n_heads=4,\n",
    ")\n",
    "summarise('WideModelAttention (CIMATT)', model_cimatt, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. MCIANet (ATT — Multi-Channel Image Analysis Network)\n",
    "\n",
    "**Key design:** Per-channel **shared** stem (stride-2, same weights across channels) + **SpatialFocusMap** (learnable Gaussian-initialised spatial attention that suppresses background outside the cell) + two stages of joint convolutional processing interspersed with **cross-channel attention** at 6×6 and 3×3 spatial resolutions.\n",
    "\n",
    "**Key novelties over CIMATT:**\n",
    "- Shared stem (parameter-efficient, assumes all channels have similar spatial statistics)\n",
    "- SpatialFocusMap: explicitly learns to focus on the cell body and ignore neighbouring cells\n",
    "- Attention is applied at multiple spatial scales (early and late)\n",
    "- Joint convolutional stages after the stem allow cross-channel spatial reasoning\n",
    "\n",
    "**Inductive bias:** Spatial filtering of the cell body is the first priority; cross-channel co-expression reasoning happens iteratively at decreasing resolutions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_att = MCIANet(\n",
    "    in_channels=N_CHANNELS,\n",
    "    input_size=INPUT_SIZE,\n",
    "    stem_dim=STEM_WIDTH,\n",
    "    n_heads=4,\n",
    "    stem_blocks=2,\n",
    "    stage1_blocks=2,\n",
    "    stage2_blocks=2,\n",
    "    expansion=2,\n",
    "    drop_prob=0.05,\n",
    "    sigma_fraction=0.35,\n",
    "    spatial_init_mode='ones',\n",
    ")\n",
    "summarise('MCIANet (ATT)', model_att, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Early Fusion Baselines\n",
    "These models mix all input channels together in the first (or zeroth) layer. They serve as the null hypothesis: if channel separability provides no benefit, these should match or exceed the channel-separable models above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. EarlyFusionModel\n",
    "\n",
    "**Key design:** Identical architecture to WideModel (CIM) in every respect — same stem_width, block_width, layer_config, number of parameters — **except** all Conv2d operations use `groups=1` (standard convolutions) instead of `groups=in_channels` (depthwise). All 41 channels are mixed together in the very first 3×3 convolution.\n",
    "\n",
    "**This is the cleanest possible ablation:** a single hyperparameter change (`groups`) separates this model from WideModel. Any performance difference is directly attributable to the depthwise vs. standard convolution choice.\n",
    "\n",
    "**Null hypothesis:** If EarlyFusionModel ≥ WideModel → channel separability provides no benefit for this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ef = EarlyFusionModel(\n",
    "    in_channels=N_CHANNELS,\n",
    "    stem_width=STEM_WIDTH,\n",
    "    block_width=2,\n",
    "    layer_config=[1, 1],\n",
    "    late_fusion=False,\n",
    "    drop_prob=0.05,\n",
    ")\n",
    "summarise('EarlyFusionModel', model_ef, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. MidFusionModel\n",
    "\n",
    "**Key design:** The stem is **channel-separable** (depthwise conv, identical to WideModel), but all subsequent ConvBlocks use **standard convolutions** that freely mix channels. Fusion happens after the first layer of per-channel spatial feature extraction.\n",
    "\n",
    "**Research question:** Is one layer of per-channel spatial processing sufficient to capture the per-marker spatial statistics, after which standard cross-channel processing is fine? Or does maintaining separability throughout the network matter?\n",
    "\n",
    "**Expected outcome:** Should fall between WideModel and EarlyFusionModel. If MidFusion ≈ WideModel, the benefit of separability comes mainly from the stem. If MidFusion ≈ EarlyFusion, the benefit requires separability in the processing layers too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_mf = MidFusionModel(\n",
    "    in_channels=N_CHANNELS,\n",
    "    stem_width=STEM_WIDTH,\n",
    "    block_width=2,\n",
    "    layer_config=[1, 1],\n",
    "    drop_prob=0.05,\n",
    ")\n",
    "summarise('MidFusionModel', model_mf, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. ProjectionFusionModel\n",
    "\n",
    "**Key design:** A **1×1 convolution** (kernel_size=1, no spatial context) collapses all input channels into the joint feature space **before any spatial processing**. All subsequent layers operate on already-fused features.\n",
    "\n",
    "**This is the most aggressive early fusion strategy:** unlike EarlyFusionModel (which uses a 3×3 stem that at least has local spatial context when mixing channels), the 1×1 projection has zero spatial receptive field. The model never has access to per-channel spatial structure.\n",
    "\n",
    "**Interpretation:** If ProjectionFusion ≈ EarlyFusion, spatial context at the fusion point doesn't matter — the damage is done simply by mixing channels. If ProjectionFusion << EarlyFusion, even a 3×3 mixed-channel conv is harmful, and the order of spatial↔channel mixing matters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_pf = ProjectionFusionModel(\n",
    "    in_channels=N_CHANNELS,\n",
    "    stem_width=STEM_WIDTH,\n",
    "    block_width=2,\n",
    "    layer_config=[1, 1],\n",
    "    drop_prob=0.05,\n",
    ")\n",
    "summarise('ProjectionFusionModel', model_pf, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. ResNetBaseline\n",
    "\n",
    "**Key design:** A lightweight ResNet-style architecture with standard early-fusion convolutions, scaled for small patches (no strided conv at input, 2 residual stages only). Takes all `in_channels` as a standard multi-channel input with no assumptions about channel structure.\n",
    "\n",
    "**Role:** External reference point outside the WideModel family. A well-understood architecture that makes no assumptions about channel separability whatsoever and treats the multiplex image identically to how a standard CNN treats an RGB image.\n",
    "\n",
    "**Note:** Output dimension is `base_width * 4 = 256`, independent of `n_channels`. The neck `in_channels` must be set to 256 in the experiment config."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_rn = ResNetBaseline(\n",
    "    in_channels=N_CHANNELS,\n",
    "    base_width=64,\n",
    "    drop_prob=0.05,\n",
    ")\n",
    "summarise('ResNetBaseline', model_rn, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\n",
    "    ('WideModel (CIM)',          model_cim,    'Channel-separable throughout',              'Proposed'),\n",
    "    ('SharedStemModel (SSM)',    model_ssm,    'Shared stem, channel-separable',            'Proposed variant'),\n",
    "    ('WideModelAttention (CIMATT)', model_cimatt, 'Channel-separable + cross-channel attn',    'Proposed + attention'),\n",
    "    ('MCIANet (ATT)',            model_att,    'Shared stem + SpatialFocus + multi-scale attn', 'Proposed + focus'),\n",
    "    ('EarlyFusionModel',        model_ef,     'Standard convs (groups=1) throughout',      'Ablation'),\n",
    "    ('MidFusionModel',          model_mf,     'Depthwise stem, then standard convs',       'Ablation'),\n",
    "    ('ProjectionFusionModel',   model_pf,     '1x1 pixel fusion first, then standard convs','Ablation'),\n",
    "    ('ResNetBaseline',          model_rn,     'Standard ResNet, no channel assumptions',   'External baseline'),\n",
    "]\n",
    "\n",
    "print(f\"{'Model':<30} {'Params':>10}  {'Fusion strategy':<45} {'Role'}\")\n",
    "print('-' * 110)\n",
    "for name, model, strategy, role in models:\n",
    "    n_params = sum(p.numel() for p in model.parameters())\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        out = model(x)[0]\n",
    "    print(f\"{name:<30} {n_params:>10,}  {strategy:<45} {role}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
